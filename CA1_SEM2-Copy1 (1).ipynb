{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67ca7143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c999a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa3384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due the different version of python can be found and each machine or libriries \n",
    "# Avoid warnigs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437af6e6",
   "metadata": {},
   "source": [
    "# 2. Data Understanding Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112ff28",
   "metadata": {},
   "source": [
    "Practical Big Data (PySparkSQL) - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e3691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PySparkSQL\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession        \n",
    "\n",
    "# Create SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkSQL\")\n",
    "  .getOrCreate())\n",
    "\n",
    "# Path to dataset\n",
    "csv_file = \"/user/user1/Maternal_Health_Risk.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87be1ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read and create a temporary view\n",
    "df = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(csv_file))\n",
    "df.createOrReplaceTempView(\"rawdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5e366f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- SystolicBP: integer (nullable = true)\n",
      " |-- DiastolicBP: integer (nullable = true)\n",
      " |-- BS: double (nullable = true)\n",
      " |-- BodyTemp: double (nullable = true)\n",
      " |-- HeartRate: integer (nullable = true)\n",
      " |-- RiskLevel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualise inferred schema\n",
    "data = spark.sql(\"SELECT * FROM rawdata\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f53d1244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+----+--------+---------+---------+\n",
      "|Age|SystolicBP|DiastolicBP|  BS|BodyTemp|HeartRate|RiskLevel|\n",
      "+---+----------+-----------+----+--------+---------+---------+\n",
      "| 25|       130|         80|15.0|    98.0|       86|high risk|\n",
      "| 35|       140|         90|13.0|    98.0|       70|high risk|\n",
      "| 29|        90|         70| 8.0|   100.0|       80|high risk|\n",
      "| 30|       140|         85| 7.0|    98.0|       70|high risk|\n",
      "| 35|       120|         60| 6.1|    98.0|       76| low risk|\n",
      "+---+----------+-----------+----+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display 5 first rows\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "47e8bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover a coluna 'person_id'\n",
    "data = data.drop('person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "555dbdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1014|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "data.createOrReplaceTempView(\"RiskLevel\")\n",
    "\n",
    "# Get the count of rows using SQL\n",
    "spark.sql(\"SELECT COUNT(1) FROM RiskLevel\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4b5a40b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|RiskLevel|count|\n",
      "+---------+-----+\n",
      "| low risk|  406|\n",
      "| mid risk|  336|\n",
      "|high risk|  272|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Counting the occurrences of each value in the \"RiskLevel\" column\n",
    "result = spark.sql(\"SELECT RiskLevel, COUNT(*) AS count FROM RiskLevel GROUP BY RiskLevel\")\n",
    "\n",
    "# Displaying the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63966163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Age: 70\n",
      "Minimum Age: 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Finding the maximum and minimum age\n",
    "max_age = data.select(max(\"Age\")).collect()[0][0]\n",
    "min_age = data.select(min(\"Age\")).collect()[0][0]\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Maximum Age:\", max_age)\n",
    "print(\"Minimum Age:\", min_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d277dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+--------------+--------------+\n",
      "|RiskLevel|MaxSystolicBP|MinSystolicBP|MaxDiastolicBP|MinDiastolicBP|\n",
      "+---------+-------------+-------------+--------------+--------------+\n",
      "| low risk|          129|           70|           100|            49|\n",
      "| mid risk|          140|           70|           100|            50|\n",
      "|high risk|          160|           83|           100|            60|\n",
      "+---------+-------------+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Grouping by RiskLevel and calculating maximum and minimum values for SystolicBP and DiastolicBP\n",
    "risk_stats = data.groupBy(\"RiskLevel\").agg(\n",
    "    max(\"SystolicBP\").alias(\"MaxSystolicBP\"),\n",
    "    min(\"SystolicBP\").alias(\"MinSystolicBP\"),\n",
    "    max(\"DiastolicBP\").alias(\"MaxDiastolicBP\"),\n",
    "    min(\"DiastolicBP\").alias(\"MinDiastolicBP\")\n",
    ")\n",
    "\n",
    "# Displaying the results\n",
    "risk_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfee7ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+---------+\n",
      "|Age|SystolicBP|DiastolicBP|RiskLevel|\n",
      "+---+----------+-----------+---------+\n",
      "| 25|       130|         80|high risk|\n",
      "| 35|       140|         90|high risk|\n",
      "| 29|        90|         70|high risk|\n",
      "| 30|       140|         85|high risk|\n",
      "| 23|       140|         80|high risk|\n",
      "| 35|        85|         60|high risk|\n",
      "| 42|       130|         80|high risk|\n",
      "| 50|       140|         90|high risk|\n",
      "| 25|       140|        100|high risk|\n",
      "| 40|       140|        100|high risk|\n",
      "| 48|       140|         90|high risk|\n",
      "| 25|       140|        100|high risk|\n",
      "| 23|       140|         90|high risk|\n",
      "| 34|        85|         60|high risk|\n",
      "| 50|       140|         90|high risk|\n",
      "| 25|       140|        100|high risk|\n",
      "| 42|       140|        100|high risk|\n",
      "| 32|       140|        100|high risk|\n",
      "| 50|       140|         95|high risk|\n",
      "| 38|       135|         60|high risk|\n",
      "+---+----------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter records with high risk\n",
    "high_risk_data = data.filter(col(\"RiskLevel\") == \"high risk\")\n",
    "\n",
    "# Show ages, all types of pressure, and risk level with highest risk\n",
    "high_risk_data.select(\"Age\", \"SystolicBP\", \"DiastolicBP\", \"RiskLevel\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66ab31",
   "metadata": {},
   "source": [
    "# 3. Data Preparation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f82a7",
   "metadata": {},
   "source": [
    "missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "016803c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+---+--------+---------+---------+\n",
      "|Age|SystolicBP|DiastolicBP| BS|BodyTemp|HeartRate|RiskLevel|\n",
      "+---+----------+-----------+---+--------+---------+---------+\n",
      "|  0|         0|          0|  0|       0|        0|        0|\n",
      "+---+----------+-----------+---+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_values = data.select([col(c).isNull().cast(\"int\").alias(c) for c in data.columns]) \\\n",
    "                     .agg(*[F.sum(c).alias(c) for c in data.columns])\n",
    "\n",
    "# Show the number of missing values in each column\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c4afef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  SystolicBP  DiastolicBP    BS  BodyTemp  HeartRate  RiskLevel\n",
      "0   25         130           80  15.0      98.0         86  high risk\n",
      "1   35         140           90  13.0      98.0         70  high risk\n",
      "2   29          90           70   8.0     100.0         80  high risk\n",
      "3   30         140           85   7.0      98.0         70  high risk\n",
      "4   35         120           60   6.1      98.0         76   low risk\n"
     ]
    }
   ],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = data.toPandas()\n",
    "\n",
    "# Display the first few rows of the Pandas DataFrame\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e10e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aef2c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Modeling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78296e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab97ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867f940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
